{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This project is an introduction to deep learning for image classification using [Keras](https://keras.io/) and [Tensorflow](https://www.tensorflow.org/) and the [Sequential API](https://keras.io/guides/sequential_model/). In it, I build and optimize a multi-layer perceptron feed-forward neural network to classify images from the [KMINST](http://codh.rois.ac.jp/kmnist/index.html.en) dataset. Then, I implement a convolutional neural network and optimize its layers and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, Rescaling\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adam\n",
    "\n",
    "from os import cpu_count\n",
    "\n",
    "plt.style.use('seaborn-dark')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download, Load, Visualize, and Reshape the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('data/kmnist-train-imgs.npz')['arr_0']\n",
    "y_train = np.load('data/kmnist-train-labels.npz')['arr_0']\n",
    "X_test = np.load('data/kmnist-test-imgs.npz')['arr_0']\n",
    "y_test = np.load('data/kmnist-test-labels.npz')['arr_0']\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the [MNIST](https://www.tensorflow.org/datasets/catalog/mnist) dataset, we have 60,000 images in the training set and 10,000 images in the test set. Each image is 28x28 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, just as in the MNIST dataset, the images are grayscale with a value of 0-255 indicating the darkness of the pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(y_train)\n",
    "plt.title('Training Labels Histogram', fontsize=20)\n",
    "plt.xlabel('Class', fontsize=15)\n",
    "plt.ylabel('Count', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our labels are 0-9 and we have a very balanced training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samples of each class\n",
    "Let's look at several samples of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 10\n",
    "label_sample_indexes = dict()\n",
    "for label in set(y_train):\n",
    "    label_sample_indexes[label] = [i for i,x in enumerate(y_train) if x==label][0:samples]\n",
    "label_sample_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = label_sample_indexes.keys()\n",
    "_, axes = plt.subplots(nrows=len(labels), ncols=samples, figsize=(10, 20))\n",
    "for ax_row, label in zip(axes, labels):\n",
    "    for ax, image in zip(ax_row, X_train[label_sample_indexes[label]]):\n",
    "        ax.set_axis_off()\n",
    "        ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "        ax.set_title(f\"Training: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping the Data\n",
    "\n",
    "Here I will flatten the image itself into a 1D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_shape = X_train.shape\n",
    "X_train_flattened = X_train.reshape(train_shape[0], train_shape[1] * train_shape[2])\n",
    "test_shape = X_test.shape\n",
    "X_test_flattened = X_test.reshape(test_shape[0], test_shape[1] * test_shape[2])\n",
    "\n",
    "print(X_train_flattened.shape, X_test_flattened.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "X_train_scaled = scaler.fit_transform(X_train_flattened)\n",
    "X_test_scaled = scaler.transform(X_test_flattened)\n",
    "print(\"Average:\", np.mean(X_train_scaled[:, 350]))\n",
    "print(\"Std dev:\", np.std(X_train_scaled[:, 350]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Logistic Regression\n",
    "\n",
    "As in the example, first I'll perform a logistic regression classification to compare, later, with the deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDClassifier(loss=\"log\", n_jobs=cpu_count())\n",
    "sgd.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_predicted = sgd.predict(X_test_scaled)\n",
    "\n",
    "acc = 100. * accuracy_score(y_test, y_predicted)\n",
    "cm = confusion_matrix(y_test, y_predicted)\n",
    "\n",
    "print(f\"Accuracy: {acc:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression performed with an accuracy of about 65%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Multi-Layer Perceptron\n",
    "\n",
    "For this multiclass classification problem I will choose the [sparse_categorical_crossentropy](https://keras.io/api/losses/probabilistic_losses/#sparsecategoricalcrossentropy-class) for the probabilistic loss function since we have a multiclass problem and our classes are represented as integers (i.e., not one-hot encoded). I'll also begin with the [SGD](https://keras.io/api/optimizers/sgd/) optimizer as I have worked with this the most in class so far.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_hidden=2, n_neurons=8, dropout=None, loss='sparse_categorical_crossentropy', optimizer=SGD()):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(28, 28)))\n",
    "    model.add(Rescaling(scale=1./255))\n",
    "    model.add(Flatten())\n",
    "    for hidden_layer in range(n_hidden):\n",
    "        model.add(Dense(n_neurons, activation='relu'))\n",
    "        if dropout:\n",
    "            model.add(Dropout(dropout))\n",
    "    model.add(Dense(len(labels), activation='softmax'))\n",
    "    model.summary()\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_predict(model, batch_size=256, epochs=10):\n",
    "    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs)\n",
    "    predicted_probabilities = model.predict(X_test)\n",
    "    predicted_classes = np.argmax(predicted_probabilities, axis=1)\n",
    "    acc = 100. * accuracy_score(y_test, predicted_classes)\n",
    "    cm = confusion_matrix(y_test, predicted_classes)\n",
    "\n",
    "    return acc, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, cm = fit_and_predict(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: {acc:.2f}%\")\n",
    "print(f\"Confusion Matrix:\\n{cm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_model_params = {\n",
    "    'n_hidden': [0, 1, 2],\n",
    "    'n_neurons': [32, 128, 512],\n",
    "    'dropout': [.1, .2],\n",
    "    'optimizer': [SGD(), RMSprop(), Adam()]\n",
    "}\n",
    "fit_and_predict_params = {\n",
    "    'epochs': [25, 50]\n",
    "}\n",
    "\n",
    "keys, values = zip(*build_model_params.items())\n",
    "build_model_kwargses = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "keys, values = zip(*fit_and_predict_params.items())\n",
    "fit_and_predict_kwargses = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "results = []\n",
    "for build_model_kwargs in build_model_kwargses:\n",
    "    for fit_and_predict_kwargs in fit_and_predict_kwargses:\n",
    "        model = build_model(**build_model_kwargs)\n",
    "        acc, cm = fit_and_predict(model, **fit_and_predict_kwargs)\n",
    "        results.append((build_model_kwargs, fit_and_predict_kwargs, acc, cm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "for result in results:\n",
    "    acc = result[2]\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_build_model_params = result[0]\n",
    "        best_fit_and_predict_params = result[1]\n",
    "        best_cm = result[3]\n",
    "    print(f\"{result[0]} {result[1]} accuracy - {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nBest accuracy: {best_acc:.2f}% - {best_build_model_params} {best_fit_and_predict_params}\\n\")\n",
    "print(\"Best Confusion Matrix:\")\n",
    "print(best_cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More hidden layers, more neurons per layer, and the [Adam]() optimizer performed the best. Accuracy is now nearly 93%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Convolutional Neural Network\n",
    "\n",
    "For this, I will begin with my best parameters from the multi-layer perceptron model: 2 hidden Dense layers, 512 neurons per dense layer, with a .2 dropout and the [Adam](https://keras.io/api/optimizers/adam/) optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_model(\n",
    "    n_filters=[32, 64], kernel_size=(3, 3), n_hidden=2, pooling=MaxPooling2D(pool_size=(2, 2)),\n",
    "    n_neurons=512, dropout=.2, loss='sparse_categorical_crossentropy', optimizer=Adam()):\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Input(shape=(28, 28, 1)))\n",
    "    model.add(Rescaling(scale=1./255))\n",
    "    \n",
    "    for filters in n_filters:\n",
    "        model.add(Conv2D(filters=filters, kernel_size=kernel_size))\n",
    "\n",
    "    if pooling:\n",
    "        model.add(pooling)\n",
    "    \n",
    "    if pooling is None or type(pooling) is MaxPooling2D:\n",
    "        model.add(Flatten())\n",
    "\n",
    "    for hidden_layer in range(n_hidden):\n",
    "        model.add(Dense(n_neurons, activation='relu'))\n",
    "        if dropout:\n",
    "            model.add(Dropout(dropout))\n",
    "    model.add(Dense(len(labels), activation='softmax'))\n",
    "    model.summary()\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_cnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, cm = fit_and_predict(model, batch_size=128, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: {acc:.2f}%\")\n",
    "print(f\"Confusion Matrix:\\n{cm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize the CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_model_params = {\n",
    "    'n_filters': [[16, 32], [32, 64], [64, 128]],\n",
    "    'kernel_size': [(3, 3), (5, 5), (7, 7)],\n",
    "    'pooling': [None, MaxPooling2D(pool_size=(2, 2)), GlobalMaxPooling2D()],\n",
    "    'optimizer': [SGD(), RMSprop(), Adam()]\n",
    "}\n",
    "fit_and_predict_params = {\n",
    "    'batch_size': [128, 256],\n",
    "    'epochs': [10, 25]\n",
    "}\n",
    "\n",
    "keys, values = zip(*build_model_params.items())\n",
    "build_model_kwargses = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "keys, values = zip(*fit_and_predict_params.items())\n",
    "fit_and_predict_kwargses = [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "results = []\n",
    "for build_model_kwargs in build_model_kwargses:\n",
    "    for fit_and_predict_kwargs in fit_and_predict_kwargses:\n",
    "        model = build_cnn_model(**build_model_kwargs)\n",
    "        acc, cm = fit_and_predict(model, **fit_and_predict_kwargs)\n",
    "        results.append((build_model_kwargs, fit_and_predict_kwargs, acc, cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0\n",
    "for result in results:\n",
    "    acc = result[2]\n",
    "    if acc > best_acc:\n",
    "        best_acc = acc\n",
    "        best_build_model_params = result[0]\n",
    "        best_fit_and_predict_params = result[1]\n",
    "        best_cm = result[3]\n",
    "    print(f\"{result[0]} {result[1]} accuracy - {acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nBest accuracy: {best_acc:.2f}% - {best_build_model_params} {best_fit_and_predict_params}\\n\")\n",
    "print(\"Best Confusion Matrix:\")\n",
    "print(best_cm)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "32a282a87d2449fec7e9b04fa9cf8e162c47c13b3b2e5daacb2cbdfd25191a73"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
