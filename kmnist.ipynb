{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout, Rescaling\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adam, Adadelta, Adamax, Nadam, Ftrl\n",
    "\n",
    "from os import cpu_count\n",
    "\n",
    "plt.style.use('seaborn-dark')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download, Load, Visualize, and Reshape the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('data/kmnist-train-imgs.npz')['arr_0']\n",
    "y_train = np.load('data/kmnist-train-labels.npz')['arr_0']\n",
    "X_test = np.load('data/kmnist-test-imgs.npz')['arr_0']\n",
    "y_test = np.load('data/kmnist-test-labels.npz')['arr_0']\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the [MNIST](https://www.tensorflow.org/datasets/catalog/mnist) dataset, we have 60,000 images in the training set and 10,000 images in the test set. Each image is 28x28 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, just as in the MNIST dataset, the images are grayscale with a value of 0-255 indicating the darkness of the pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(y_train)\n",
    "plt.title('Training Labels Histogram', fontsize=20)\n",
    "plt.xlabel('Class', fontsize=15)\n",
    "plt.ylabel('Count', fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our labels are 0-9 and we have a very balanced training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Samples of each class\n",
    "Let's look at several samples of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 10\n",
    "label_sample_indexes = dict()\n",
    "for label in set(y_train):\n",
    "    label_sample_indexes[label] = [i for i,x in enumerate(y_train) if x==label][0:samples]\n",
    "label_sample_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = label_sample_indexes.keys()\n",
    "_, axes = plt.subplots(nrows=len(labels), ncols=samples, figsize=(10, 20))\n",
    "for ax_row, label in zip(axes, labels):\n",
    "    for ax, image in zip(ax_row, X_train[label_sample_indexes[label]]):\n",
    "        ax.set_axis_off()\n",
    "        ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "        ax.set_title(f\"Training: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping the Data\n",
    "\n",
    "Here I will flatten the image itself into a 1D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_shape = X_train.shape\n",
    "X_train_flattened = X_train.reshape(train_shape[0], train_shape[1] * train_shape[2])\n",
    "test_shape = X_test.shape\n",
    "X_test_flattened = X_test.reshape(test_shape[0], test_shape[1] * test_shape[2])\n",
    "\n",
    "print(X_train_flattened.shape, X_test_flattened.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(with_mean=False, with_std=True)\n",
    "X_train_scaled = scaler.fit_transform(X_train_flattened)\n",
    "X_test_scaled = scaler.transform(X_test_flattened)\n",
    "print(\"Average:\", np.mean(X_train_scaled[:, 350]))\n",
    "print(\"Std dev:\", np.std(X_train_scaled[:, 350]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Logistic Regression\n",
    "\n",
    "As in the example, first I'll perform a logistic regression classification to compare, later, with the deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDClassifier(loss=\"log\", n_jobs=cpu_count())\n",
    "sgd.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_predicted = sgd.predict(X_test_scaled)\n",
    "\n",
    "acc = 100. * accuracy_score(y_test, y_predicted)\n",
    "cm = confusion_matrix(y_test, y_predicted)\n",
    "\n",
    "print(f\"Accuracy: {acc:.2f}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logistic regression performed with an accuracy of about 65%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Multi-Layer Perceptron\n",
    "\n",
    "For this multiclass classification problem I will choose the [sparse_categorical_crossentropy](https://keras.io/api/losses/probabilistic_losses/#sparsecategoricalcrossentropy-class) for the probabilistic loss function since we have a multiclass problem and our classes are represented as integers (i.e., not one-hot encoded). I'll also begin with the [SGD](https://keras.io/api/optimizers/sgd/) optimizer as I have worked with this the most in class so far.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(28, 28)))\n",
    "model.add(Rescaling(scale=1./255))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(len(labels), activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy', \n",
    "    optimizer=SGD(),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(n_hidden=2, n_neurons=8, dropout=None, loss='sparse_categorical_crossentropy', optimizer=SGD()):\n",
    "    model = Sequential()\n",
    "    model.add(Input(shape=(28, 28)))\n",
    "    model.add(Rescaling(scale=1./255))\n",
    "    model.add(Flatten())\n",
    "    for hidden_layer in range(n_hidden):\n",
    "        model.add(Dense(n_neurons, activation='relu'))\n",
    "        if dropout:\n",
    "            model.add(Dropout(dropout))\n",
    "    model.add(Dense(len(labels), activation='softmax'))\n",
    "    model.summary()\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_predict(batch_size=256, epochs=10):\n",
    "    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs)\n",
    "    predicted_probabilities = model.predict(X_test)\n",
    "    predicted_classes = np.argmax(predicted_probabilities, axis=1)\n",
    "    acc = 100. * accuracy_score(y_test, predicted_classes)\n",
    "    cm = confusion_matrix(y_test, predicted_classes)\n",
    "\n",
    "    return acc, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc, cm = fit_and_predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: {acc:.2f}%\")\n",
    "print(f\"Confusion Matrix:\\n{cm}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "32a282a87d2449fec7e9b04fa9cf8e162c47c13b3b2e5daacb2cbdfd25191a73"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
